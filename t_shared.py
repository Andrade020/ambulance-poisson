# -*- coding: utf-8 -*-
"""T_shared.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OrG382QkccIP_h3B76nSDstCvYp-MFqp
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

#@title Simulando para comprovar probabilidade Multinomial
import numpy as np
from scipy.stats import poisson, multinomial
from collections import Counter

# Definindo parâmetros de lambda para as distribuições de Poisson
lambda_1, lambda_2, lambda_3 = 3, 4, 2
num_simulations = 10000000  # Número de simulações para garantir boa aproximação

# Simulando as variáveis de Poisson
x1_samples = poisson.rvs(lambda_1, size=num_simulations)
x2_samples = poisson.rvs(lambda_2, size=num_simulations)
x3_samples = poisson.rvs(lambda_3, size=num_simulations)

# Calculando a soma
s_samples = x1_samples + x2_samples + x3_samples

# Filtrando amostras com soma igual a uma constante (por exemplo, S = 10)
S_value = 10
filtered_indices = np.where(s_samples == S_value)[0]

# Contagem de eventos (X1, X2, X3) para essas amostras
filtered_x1 = x1_samples[filtered_indices]
filtered_x2 = x2_samples[filtered_indices]
filtered_x3 = x3_samples[filtered_indices]

# Contando as ocorrências (X1, X2, X3)
event_counts = Counter(zip(filtered_x1, filtered_x2, filtered_x3))

# Calculando a distribuição multinomial teórica para o mesmo valor de S
total_events = len(filtered_indices)  # Número de eventos com S = 10
prob_1 = lambda_1 / (lambda_1 + lambda_2 + lambda_3)
prob_2 = lambda_2 / (lambda_1 + lambda_2 + lambda_3)
prob_3 = lambda_3 / (lambda_1 + lambda_2 + lambda_3)

# Calculando a probabilidade teórica usando a fórmula multinomial
theoretical_probs = {}
for (k1, k2, k3), count in event_counts.items():
    theoretical_prob = multinomial.pmf([k1, k2, k3], S_value, [prob_1, prob_2, prob_3])
    theoretical_probs[(k1, k2, k3)] = theoretical_prob

# Comparação entre as probabilidades simuladas e teóricas
simulated_probs = {k: v / total_events for k, v in event_counts.items()}

# Unindo ambas as probabilidades para comparação
comparison = {k: (simulated_probs.get(k, 0), theoretical_probs.get(k, 0)) for k in set(simulated_probs) | set(theoretical_probs)}
comparison

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson
##################################################################################

# confgr do modlo
np.random.seed(42)  # para reprdt
###############################################################################################

# parmtr verddr
lambda_true = np.array([3.0, 4.0, 5.0])  # taxs de possn
p_true = np.array([0.6, 0.5, 0.7])  # probbl para x_i
############################################################################################

# numro de amostr
n_samples = 100
########################################################################################

# funco para gerr dads
def generate_data(lambda_true, p_true, n_samples):
    X = []
    Y = []
    for i in range(len(lambda_true)):
        X_i = poisson.rvs(p_true[i] * lambda_true[i], size=n_samples)
        Y_i = poisson.rvs((1 - p_true[i]) * lambda_true[i], size=n_samples)
        X.append(X_i)
        Y.append(Y_i)
    X = np.array(X)
    Y = np.array(Y)
    S = np.sum(X, axis=0)
    return X, Y, S
###########################################################################

# gerr dads simlds
X, Y, S = generate_data(lambda_true, p_true, n_samples)
#############################################################################################

# funco do algrtm em com rastrm da evolc dos parmtr
def em_algorithm(Y, S, p_true, max_iter=100, tol=1e-6):
    n = len(p_true)
    lambda_est = np.ones(n)  # iniclz lambda com 1
    lambda_history = [lambda_est.copy()]  # lista para armznr histrc das estmtv
###################################################################################################

    for _ in range(max_iter):
        # etpa e: calclr a esprnc condcn
        X_conditional_mean = np.array([
            S * (p_true[i] * lambda_est[i]) / np.sum(p_true * lambda_est)
            for i in range(n)
        ])
################################################################################################

        # etpa m: atulzr lambda
        lambda_new = np.array([
            np.mean(X_conditional_mean[i] + Y[i])
            for i in range(n)
        ])
##################################################################

        # adicnr a nova estmtv ao histrc
        lambda_history.append(lambda_new.copy())
###############################################################################################

        # verfcr convrg
        if np.all(np.abs(lambda_new - lambda_est) < tol):
            break
#######################################################################

        lambda_est = lambda_new
###########################################################################

    return lambda_est, lambda_history
####################################################################

# aplcr o algrtm em
lambda_estimated, lambda_history = em_algorithm(Y, S, p_true)
######################################################################################

# pltr a evolc das estmtv
plt.figure(figsize=(10, 6))
for i in range(len(lambda_true)):
    plt.plot([l[i] for l in lambda_history], label=f'?{i+1}', marker='o')
#################################################################

plt.axhline(y=lambda_true[0], color='r', linestyle='--', label='True ?1')
plt.axhline(y=lambda_true[1], color='g', linestyle='--', label='True ?2')
plt.axhline(y=lambda_true[2], color='b', linestyle='--', label='True ?3')
##############################################################

plt.xlabel('Iteration')
plt.ylabel('Estimated Parameters')
plt.title('Evolution of Parameter Estimates Over Iterations')
plt.legend()
plt.grid(True)
plt.show()
#################################################################################################

print("Parâmetros verdadeiros:", lambda_true)
print("Parâmetros estimados:", lambda_estimated)

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson
############################################################

# confgr do modlo
np.random.seed(42)  # para reprdt
#################################################################################

# parmtr verddr
lambda_true = np.array([3.0, 4.0, 5.0])  # taxs de possn
p = 0.6  # probbl para x_i (sclr)
############################################################################################

# numro de amostr
n_samples = 100
################################################################################################

# funco para gerr dads
def generate_data(lambda_true, p, n_samples):
    Y = []
    for i in range(len(lambda_true)):
        Y_i = poisson.rvs((1 - p) * lambda_true[i], size=n_samples)
        Y.append(Y_i)
    Y = np.array(Y)
    S_true = np.sum(lambda_true)
    Z = poisson.rvs(p * S_true, size=n_samples)
    return Y, Z
#####################################################################

# gerr dads simlds
Y, Z = generate_data(lambda_true, p, n_samples)
#################################################################

# funco do algrtm em com rastrm da evolc dos parmtr
def em_algorithm(Y, Z, max_iter=100, tol=1e-6):
    n = len(Y)  # numro de compnn (i)
    n_samples = Y.shape[1]  # numro de amostr (n)
    lambda_est = np.ones(n)  # iniclz lambda com 1
    lambda_history = [lambda_est.copy()]  # lista para armznr histrc das estmtv
#############################################################################

    for _ in range(max_iter):
        S_est = np.sum(lambda_est)  # somtr das estmtv lambda atus
####################################################################

        # etpa e: calclr a esprnc condcn
        E_X = np.array([
            Z * lambda_est[i] / S_est
            for i in range(n)
        ])  # e_x tem shpe (n, n_smpl)
##################################################################################################

        # etpa m: atulzr lambda
        lambda_new = np.array([
            np.mean(E_X[i] + Y[i])
            for i in range(n)
        ])
####################################################################################

        # adicnr a nova estmtv ao histrc
        lambda_history.append(lambda_new.copy())
############################################################################################

        # verfcr convrg
        if np.all(np.abs(lambda_new - lambda_est) < tol):
            break
##############################################################################################

        lambda_est = lambda_new
###################################################################

    return lambda_est, lambda_history
#####################################################################################

# aplcr o algrtm em
lambda_estimated, lambda_history = em_algorithm(Y, Z)
################################################################

# pltr a evolc das estmtv
plt.figure(figsize=(10, 6))
colors = ['r', 'g', 'b']
for i in range(len(lambda_true)):
    plt.plot([l[i] for l in lambda_history], label=f'?{i+1}', marker='o')
    plt.axhline(y=lambda_true[i], color=colors[i], linestyle='--', label=f'True ?{i+1}')
######################################################################################

plt.xlabel('Iteração')
plt.ylabel('Estimativa dos Parâmetros')
plt.title('Evolução das Estimativas dos Parâmetros ao Longo das Iterações')
plt.legend()
plt.grid(True)
plt.show()
#################################################################################################

print("Parâmetros verdadeiros:", lambda_true)
print("Parâmetros estimados:", lambda_estimated)

len(p_true)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import poisson
################################################################################################

n_variables = 8
lambda_true =  np.array([8.0, 2.0, 1.0, 4.5, 8.5, 4.5, 1.5, 1.3]) #np.linspc(2, 5, n_vrbl)  # taxs de possn varnd de 2 a 5
p_true = np.random.uniform(0.1, 0.9, n_variables)  # probbl para x_i
n_variables= len(lambda_true)
######################################################################

# numro de amostr
n_samples = 100
################################################################

# funco para gerr dads
def generate_data(lambda_true, p_true, n_samples):
    Y = []
    S = np.zeros(n_samples)
    for i in range(len(lambda_true)):
        X_i = poisson.rvs(p_true[i] * lambda_true[i], size=n_samples)
        Y_i = poisson.rvs((1 - p_true[i]) * lambda_true[i], size=n_samples)
        Y.append(Y_i)
        S += X_i
    Y = np.array(Y)
    return Y, S
######################################################################

# gerr dads simlds
Y, S = generate_data(lambda_true, p_true, n_samples)
###############################################################################

# crir um datfrm com os dads simlds (apns y_i e s)
data = {
    f'Y{i+1}': Y[i] for i in range(n_variables)
}
data['S'] = S
##############################################################################

df = pd.DataFrame(data)
###############################################################################################

# exbr as primrs linhs do datfrm
print(df.head())

print(p_true)

# prompt: save df as csv

df.to_csv('simulated_data.csv', index=False)

# prmpt: imprt simltd
######################################################################################

import pandas as pd
#####################################################################################

# carrgr os dads do arqvo csv
df = pd.read_csv('simulated_data.csv')
###################################################################################################

# exbr as primrs linhs do datfrm
print(df.head())

# prompt: get df number of columns

num_cols = len(df.columns)
print(num_cols)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import poisson
#############################################################

# funco do algrtm em usndo o datfrm como entrda
def em_algorithm_from_df(df, p_true, max_iter=100, tol=1e-6):
    n_variables= len(df.columns)-1
    n = len(p_true)
    lambda_est = np.ones(n)  # iniclz lambda com 1
    lambda_history = [lambda_est.copy()]  # alsta para armznr histrc das estmtv
    log_likelihood_history = []  # lista para armznr histrc da verssm
##############################################################################

    Y = np.array([df[f'Y{i+1}'].values for i in range(n_variables)])
    S = df['S'].values
##################################################################################

    def log_likelihood(lambda_est):
        # calcla a log-verssm
        X_conditional_mean = np.array([
            S * (p_true[i] * lambda_est[i]) / np.sum(p_true * lambda_est)
            for i in range(n)
        ])
        return np.sum([
            np.sum(X_conditional_mean[i] * np.log(p_true[i] * lambda_est[i])
                   - p_true[i] * lambda_est[i])
            + np.sum(Y[i] * np.log((1 - p_true[i]) * lambda_est[i])
                   - (1 - p_true[i]) * lambda_est[i])
            for i in range(n)
        ])
##########################################################################

    for _ in range(max_iter):
        # etpa e: calclr a esprnc condcn
        X_conditional_mean = np.array([
            S * (p_true[i] * lambda_est[i]) / np.sum(p_true * lambda_est)
            for i in range(n)
        ])
#####################################################################################

        # etpa m: atulzr lambda
        lambda_new = np.array([
            np.mean(X_conditional_mean[i] + Y[i])
            for i in range(n)
        ])
###################################################################################

        # adicnr a nova estmtv ao histrc
        lambda_history.append(lambda_new.copy())
###############################################################

        # calclr e armznr a verssm
        log_likelihood_value = log_likelihood(lambda_new)
        log_likelihood_history.append(log_likelihood_value)
#############################################################################################

        # verfcr convrg
        if np.all(np.abs(lambda_new - lambda_est) < tol):
            break
##################################################################

        lambda_est = lambda_new
###############################################################

    return lambda_est, lambda_history, log_likelihood_history
################################################################

p_true= np.array([0.53122461, 0.47473989, 0.4704848,  0.14325829, 0.64549933, 0.80112796, 0.2255324,  0.76525063])
# aplcr o algrtm em usndo o datfrm
lambda_estimated, lambda_history, log_likelihood_history = em_algorithm_from_df(df, p_true)
###########################################################################

# pltr a evolc das estmtv e da verssm
plt.figure(figsize=(14, 7))
####################################################################

# pltr a evolc das estmtv dos parmtr
plt.subplot(1, 2, 1)
for i in range(len(lambda_estimated)):
    plt.plot([l[i] for l in lambda_history], label=f'?{i+1}', marker='o')
####################################################################################

#plt.axhlne(y=lambd_[0], colr='r', linsty='--', labl='true ?1')
#plt.axhlne(y=lambd_[-1], colr='b', linsty='--', labl='true ?20')
#################################################################################################

plt.xlabel('Iteration')
plt.ylabel('Estimated Parameters')
plt.title('Evolution of Parameter Estimates')
plt.legend()
plt.grid(True)
#####################################################################

# pltr a trajtr da verssm
plt.subplot(1, 2, 2)
plt.plot(log_likelihood_history, marker='o', color='purple')
plt.xlabel('Iteration')
plt.ylabel('Log-Likelihood')
plt.title('Log-Likelihood Over Iterations')
plt.grid(True)
####################################################################################

plt.tight_layout()
plt.show()
###################################################################

#prnt("parmtr verddr:", lambd_)
print("Parâmetros estimados:", lambda_estimated)

df

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import poisson
###############################################################

n_variables = 8
lambda_true =  np.array([8.0, 2.0, 1.0, 4.5, 8.5, 4.5, 1.5, 1.3]) #np.linspc(2, 5, n_vrbl)  # taxs de possn varnd de 2 a 5
p_true = np.random.uniform(0.1, 0.9, n_variables)  # probbl para x_i
n_variables= len(lambda_true)
#####################################################################################

# numro de amostr
n_samples = 100
#############################################################################################

# funco para gerr dads
def generate_data(lambda_true, p_true, n_samples):
    Y = []
    S = np.zeros(n_samples)
    for i in range(len(lambda_true)):
        X_i = poisson.rvs(p_true[i] * lambda_true[i], size=n_samples)
        Y_i = poisson.rvs((1 - p_true[i]) * lambda_true[i], size=n_samples)
        Y.append(Y_i)
        S += X_i
    Y = np.array(Y)
    return Y, S
###############################################################################

# gerr dads simlds
Y, S = generate_data(lambda_true, p_true, n_samples)
#########################################################################

# crir um datfrm com os dads simlds (apns y_i e s)
data = {
    f'Y{i+1}': Y[i] for i in range(n_variables)
}
data['S'] = S
##################################################################

df = pd.DataFrame(data)
######################################################################

# exbr as primrs linhs do datfrm
print(df.head())
####################################################################################

#######################################################################

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
###########################################################################

# usar os dads simlds do primr modlo
# supndo que y (onde a loclzc foi reprtd) e s (totl de evnts nao reprtd) ja form gerds
Y = np.array([df[f'Y{i+1}'].values for i in range(n_variables)])
S = df['S'].values
##########################################################################################

# numro de catgrs e zons (ajstr confrm o contxt do primr codgo)
n_intervals = Y.shape[1]  # cada amstra eh um "intrvl" no tempo
categories = 1            # uma catgr de chamds
zones = n_variables        # numro de zons eh o numro de varvs no primr modlo
######################################################################

# estmtv de p (proprc de evnts nao reprtd)
M_0 = np.sum(S)  # totl de chegds onde a loclzc nao foi reprtd (s)
M_1 = np.sum(Y)  # totl de chegds reprtd
estimated_p = M_0 / (M_0 + M_1)  # estmtv da proprc de evnts nao reprtd
#############################################################################

# estmtv de lambda
estimated_lambda = np.zeros((categories, zones, n_intervals))
########################################################################################

for i in range(zones):
    for t in range(n_intervals):
        S_t_est = S[t]  # totl de evnts nao reprtd na amstra t
        Y_i_t = Y[i, t]  # chegds reprtd para a varvl i na amstra t
        # estmr lambda consdr p
        estimated_lambda[0, i, t] = (Y_i_t + S_t_est) / (1 - estimated_p)
#######################################################################################

print("Estimativa de p:", estimated_p)
print("Estimativa de lambda:")
print(estimated_lambda)
############################################################################

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
############################################################

# funco para realzr a estmtv dos parmtr a partr do datfrm gerdo
def analytical_estimation(df, n_variables, n_samples):
    # recprr os dads de y (chamds reprtd por area) e s (soma das nao reprtd)
    Y = np.array([df[f'Y{i+1}'].values for i in range(n_variables)])
    S = df['S'].values
####################################################################################

    # estmr p (probbl de nao reprtr a loclzc)
    M_1 = np.sum(Y)  # totl de chamds reprtd (somtr de tods as ares)
    M_0 = np.sum(S)  # totl de chamds nao reprtd (somtr de s)
#######################################################################################

    # estmtv de p
    estimated_p = M_0 / (M_0 + M_1)
#######################################################################################

    # estmtv de lambda
    estimated_lambda = np.zeros(n_variables)
    for i in range(n_variables):
        total_calls_i = np.sum(Y[i] + S) / n_samples  # totl de chamds estmds para a area i
        estimated_lambda[i] = (np.sum(Y[i]) * total_calls_i) / (n_samples * total_calls_i - np.sum(S))
###############################################################################

    return estimated_p, estimated_lambda
############################################################################################

# aplcr a funco de estmtv analtc
n_variables = 8  # numro de ares (varvs)
n_samples = 100  # numro de amostr
##############################################################

estimated_p, estimated_lambda = analytical_estimation(df, n_variables, n_samples)
####################################################################################

print("Estimativa de p:", estimated_p)
print("Estimativa de lambda:", estimated_lambda)
####################################################################

###############################################################################################
# comprc de mehtrc com os parmtr verddr
##################################################################################################

# flttn os lambda verddr
lambda_true_flattened = lambda_true.flatten()
#############################################################

# calclr mae e rmse para lambda
mae_lambda = mean_absolute_error(lambda_true_flattened, estimated_lambda)
rmse_lambda = np.sqrt(mean_squared_error(lambda_true_flattened, estimated_lambda))
###########################################################################

# prnt mae e rmse
print("Erro Absoluto Médio (MAE) para lambda:", mae_lambda)
print("Erro Quadrático Médio (RMSE) para lambda:", rmse_lambda)
################################################################################################

# comprc para p
# calclr erro abslt para p
mae_p = abs(p_true.mean() - estimated_p)
#################################################################################################

# prnt erro abslt
print("Erro Absoluto para p:", mae_p)
####################################################################################################

############################################
# comprc no grfco
###############################################################################################

# plotnd os valrs verddr vs estmds para lambda
plt.figure(figsize=(12, 6))
plt.plot(estimated_lambda, 'o', label='Estimado')
plt.plot(lambda_true, 'x', label='Verdadeiro')
plt.title('Estimativa vs Valores Verdadeiros de ?')
plt.xlabel('Áreas')
plt.ylabel('Taxa de Chegada (?)')
plt.legend()
plt.grid(True)
plt.show()
######################################################################

# pltr a comprc para p
plt.figure()
plt.bar(['Valor Verdadeiro de p', 'Estimativa de p'], [p_true.mean(), estimated_p])
plt.title('Comparação de p')
plt.ylabel('Probabilidade')
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import poisson, norm
##################################################################################################

# confgr do modlo
np.random.seed(42)
#########################################################################################

# parmtr verddr
lambda_true = np.array([8.0, 2.0, 1.0, 4.5, 8.5, 4.5, 1.5, 1.3])
p_true = np.random.uniform(0.1, 0.9, len(lambda_true))
n_samples = 100
##################################################################################

# funco para gerr dads
def generate_data(lambda_true, p_true, n_samples):
    X = []
    Y = []
    for i in range(len(lambda_true)):
        X_i = poisson.rvs(p_true[i] * lambda_true[i], size=n_samples)
        Y_i = poisson.rvs((1 - p_true[i]) * lambda_true[i], size=n_samples)
        X.append(X_i)
        Y.append(Y_i)
    X = np.array(X)
    Y = np.array(Y)
    S = np.sum(X, axis=0)
    return X, Y, S
#############################################################

# gerr dads simlds
X, Y, S = generate_data(lambda_true, p_true, n_samples)
##############################################################################################

# estmnd os parmtr a partr dos dads (mehda amostr para lambda e p)
lambda_est = (X + Y).mean(axis=1)
p_est = X.mean(axis=1) / (X + Y).mean(axis=1)
################################################################

# calcln a matrz de fishr (diagnl para simplf)
Fisher_lambda = n_samples / lambda_est
Fisher_p = n_samples * (1 / (p_est * (1 - p_est)))
###########################################################################################

# varnc estmd (invrso da matrz de fishr)
var_lambda = 1 / Fisher_lambda
var_p = 1 / Fisher_p
############################################################

# calcln intrvl de confnc de 95%
z_score = norm.ppf(0.975)  # z-scre para intrvl de confnc de 95%
ci_lambda = z_score * np.sqrt(var_lambda)
ci_p = z_score * np.sqrt(var_p)
############################################################################

# plotnd os parmtr lambda com intrvl de confnc
plt.figure(figsize=(12, 6))
###################################################################################################

# grfco para os parmtr lambda
plt.subplot(1, 2, 1)
plt.errorbar(range(len(lambda_true)), lambda_est, yerr=ci_lambda, fmt='o', label='Estimativas de $\lambda$', capsize=5)
plt.plot(range(len(lambda_true)), lambda_true, 'r^', label='Valores verdadeiros de $\lambda$')
plt.xlabel('Índice do parâmetro')
plt.ylabel('Valor de $\lambda$')
plt.title('Estimativas de $\lambda$ com Intervalos de Confiança (95%)')
plt.legend()
######################################################################################

# grfco para os parmtr p
plt.subplot(1, 2, 2)
plt.errorbar(range(len(p_true)), p_est, yerr=ci_p, fmt='o', label='Estimativas de $p$', capsize=5)
plt.plot(range(len(p_true)), p_true, 'r^', label='Valores verdadeiros de $p$')
plt.xlabel('Índice do parâmetro')
plt.ylabel('Valor de $p$')
plt.title('Estimativas de $p$ com Intervalos de Confiança (95%)')
plt.legend()
#######################################################################

plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import poisson
#########################################################################################

# confgr do modlo
np.random.seed(42)  # para reprdt
###################################################################################

# parmtr verddr
n_variables = 8
lambda_true =  np.array([8.0, 2.0, 1.0, 4.5, 8.5, 4.5, 1.5, 1.3]) #np.linspc(2, 5, n_vrbl)  # taxs de possn varnd de 2 a 5
p_true = np.random.uniform(0.1, 0.9, n_variables)  # probbl para x_i
n_variables= len(lambda_true)
# numro de amostr
n_samples = 100
#################################################################

# funco para gerr dads
def generate_data(lambda_true, p_true, n_samples):
    X = []
    Y = []
    for i in range(len(lambda_true)):
        X_i = poisson.rvs(p_true[i] * lambda_true[i], size=n_samples)
        Y_i = poisson.rvs((1 - p_true[i]) * lambda_true[i], size=n_samples)
        X.append(X_i)
        Y.append(Y_i)
    X = np.array(X)
    Y = np.array(Y)
    S = np.sum(X, axis=0)
    return X, Y, S
##########################################################################################

# gerr dads simlds
X, Y, S = generate_data(lambda_true, p_true, n_samples)
#################################################################################

# crir um datfrm com os dads simlds
data = {
    f'X{i+1}': X[i] for i in range(n_variables)
}
data.update({
    f'Y{i+1}': Y[i] for i in range(n_variables)
})
data['S'] = S
#########################################################################

df = pd.DataFrame(data)
#########################################################################

# exbr as primrs linhs do datfrm
print(df.head())
########################################################################

# funco do algrtm em usndo o datfrm como entrda
def em_algorithm_from_df(df, p_true, max_iter=100, tol=1e-6):
    n = len(p_true)
    lambda_est = np.ones(n)  # iniclz lambda com 1
    lambda_history = [lambda_est.copy()]  # lista para armznr histrc das estmtv
    log_likelihood_history = []  # lista para armznr histrc da verssm
######################################################################

    Y = np.array([df[f'Y{i+1}'].values for i in range(n)])
    S = df['S'].values
###############################################################################

    def log_likelihood(lambda_est):
        # calcla a log-verssm
        X_conditional_mean = np.array([
            S * (p_true[i] * lambda_est[i]) / np.sum(p_true * lambda_est)
            for i in range(n)
        ])
        return np.sum([
            np.sum(X_conditional_mean[i] * np.log(p_true[i] * lambda_est[i])
                   - p_true[i] * lambda_est[i])
            + np.sum(Y[i] * np.log((1 - p_true[i]) * lambda_est[i])
                   - (1 - p_true[i]) * lambda_est[i])
            for i in range(n)
        ])
########################################################################

    for _ in range(max_iter):
        # etpa e: calclr a esprnc condcn
        X_conditional_mean = np.array([
            S * (p_true[i] * lambda_est[i]) / np.sum(p_true * lambda_est)
            for i in range(n)
        ])
##############################################################

        # etpa m: atulzr lambda
        lambda_new = np.array([
            np.mean(X_conditional_mean[i] + Y[i])
            for i in range(n)
        ])
##################################################################

        # adicnr a nova estmtv ao histrc
        lambda_history.append(lambda_new.copy())
#####################################################################################

        # calclr e armznr a verssm
        log_likelihood_value = log_likelihood(lambda_new)
        log_likelihood_history.append(log_likelihood_value)
############################################################

        # verfcr convrg
        if np.all(np.abs(lambda_new - lambda_est) < tol):
            break
#####################################################################################

        lambda_est = lambda_new
###############################################################################################

    return lambda_est, lambda_history, log_likelihood_history
##################################################################################

# aplcr o algrtm em usndo o datfrm
lambda_estimated, lambda_history, log_likelihood_history = em_algorithm_from_df(df, p_true)
#################################################################

# pltr a evolc das estmtv e da verssm
plt.figure(figsize=(14, 7))
##################################################################################################

# pltr a evolc das estmtv dos parmtr
plt.subplot(1, 2, 1)
for i in range(len(lambda_true)):
    plt.plot([l[i] for l in lambda_history], label=f'?{i+1}', marker='o')
##################################################################################################

plt.axhline(y=lambda_true[0], color='r', linestyle='--', label='True ?1')
plt.axhline(y=lambda_true[-1], color='b', linestyle='--', label='True ?20')
####################################################################################

plt.xlabel('Iteration')
plt.ylabel('Estimated Parameters')
plt.title('Evolution of Parameter Estimates')
plt.legend()
plt.grid(True)
##################################################################

# pltr a trajtr da verssm
plt.subplot(1, 2, 2)
plt.plot(log_likelihood_history, marker='o', color='purple')
plt.xlabel('Iteration')
plt.ylabel('Log-Likelihood')
plt.title('Log-Likelihood Over Iterations')
plt.grid(True)
########################################################################################

plt.tight_layout()
plt.show()
##############################################################

print("Parâmetros verdadeiros:", lambda_true)
print("Parâmetros estimados:", lambda_estimated)

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import poisson
########################################################################################

# confgr do modlo
 # para reprdt
################################################################

# parmtr verddr
lambda_true = np.array([3.0, 4.0, 15.0])  # taxs de possn
p_true = np.array([0.6, 0.5, 0.7])  # probbl para x_i
#####################################################################

# numro de amostr
n_samples = 100
#########################################################################

# funco para gerr dads
def generate_data(lambda_true, p_true, n_samples):
    X = []
    Y = []
    for i in range(len(lambda_true)):
        X_i = poisson.rvs(p_true[i] * lambda_true[i], size=n_samples)
        Y_i = poisson.rvs((1 - p_true[i]) * lambda_true[i], size=n_samples)
        X.append(X_i)
        Y.append(Y_i)
    X = np.array(X)
    Y = np.array(Y)
    S = np.sum(X, axis=0)
    return X, Y, S
#######################################################################

# gerr dads simlds
X, Y, S = generate_data(lambda_true, p_true, n_samples)
############################################################

# funco do algrtm em com rastrm da evolc dos parmtr e verssm
def em_algorithm(Y, S, p_true, max_iter=100, tol=1e-6):
    n = len(p_true)
    lambda_est = np.ones(n)  # iniclz lambda com 1
    lambda_history = [lambda_est.copy()]  # lista para armznr histrc das estmtv
    log_likelihood_history = []  # lista para armznr histrc da verssm
#######################################################################################

    def log_likelihood(lambda_est):
        # calcla a log-verssm
        X_conditional_mean = np.array([
            S * (p_true[i] * lambda_est[i]) / np.sum(p_true * lambda_est)
            for i in range(n)
        ])
        return np.sum([
            np.sum(X_conditional_mean[i] * np.log(p_true[i] * lambda_est[i])
                   - p_true[i] * lambda_est[i])
            + np.sum(Y[i] * np.log((1 - p_true[i]) * lambda_est[i])
                   - (1 - p_true[i]) * lambda_est[i])
            for i in range(n)
        ])
################################################################################################

    for _ in range(max_iter):
        # etpa e: calclr a esprnc condcn
        X_conditional_mean = np.array([
            S * (p_true[i] * lambda_est[i]) / np.sum(p_true * lambda_est)
            for i in range(n)
        ])
##########################################################################

        # etpa m: atulzr lambda
        lambda_new = np.array([
            np.mean(X_conditional_mean[i] + Y[i])
            for i in range(n)
        ])
#############################################################################################

        # adicnr a nova estmtv ao histrc
        lambda_history.append(lambda_new.copy())
##################################################################

        # calclr e armznr a verssm
        log_likelihood_value = log_likelihood(lambda_new)
        log_likelihood_history.append(log_likelihood_value)
##################################################################

        # verfcr convrg
        if np.all(np.abs(lambda_new - lambda_est) < tol):
            break
###################################################################################################

        lambda_est = lambda_new
##################################################################

    return lambda_est, lambda_history, log_likelihood_history
################################################################################################

# aplcr o algrtm em
lambda_estimated, lambda_history, log_likelihood_history = em_algorithm(Y, S, p_true)
#########################################################################

# pltr a evolc das estmtv
plt.figure(figsize=(14, 7))
##################################################################

# pltr a evolc das estmtv dos parmtr
plt.subplot(1, 2, 1)
for i in range(len(lambda_true)):
    plt.plot([l[i] for l in lambda_history], label=f'?{i+1}', marker='o')
#################################################################################################

plt.axhline(y=lambda_true[0], color='r', linestyle='--', label='True ?1')
plt.axhline(y=lambda_true[1], color='g', linestyle='--', label='True ?2')
plt.axhline(y=lambda_true[2], color='b', linestyle='--', label='True ?3')
############################################################################################

plt.xlabel('Iteration')
plt.ylabel('Estimated Parameters')
plt.title('Evolution of Parameter Estimates')
plt.legend()
plt.grid(True)
###################################################################################################

# pltr a trajtr da verssm
plt.subplot(1, 2, 2)
plt.plot(log_likelihood_history, marker='o', color='purple')
plt.xlabel('Iteration')
plt.ylabel('Log-Likelihood')
plt.title('Log-Likelihood Over Iterations')
plt.grid(True)
############################################################################

plt.tight_layout()
plt.show()
######################################################################################

print("Parâmetros verdadeiros:", lambda_true)
print("Parâmetros estimados:", lambda_estimated)

lambda_estimates.mean()

# prmpt: i want to plot the mse, as num_sm incrs, on log scle
################################################################################

import matplotlib.pyplot as plt
import numpy as np
#############################################################################

# defnnd parmtr de lambda para as distrb de possn
lambda_1, lambda_2, lambda_3 = 3, 4, 2
S_value = 10
#######################################################################################

# crndo lists para armznr os resltd
num_simulations_list = [1000, 10000, 100000, 1000000, 10000000]
mse_list = []
##################################################################################################

# loop para difrnt valrs de num_sm
for num_simulations in num_simulations_list:
    # simlnd as varvs de possn
    x1_samples = poisson.rvs(lambda_1, size=num_simulations)
    x2_samples = poisson.rvs(lambda_2, size=num_simulations)
    x3_samples = poisson.rvs(lambda_3, size=num_simulations)
###############################################################

    # calcln a soma
    s_samples = x1_samples + x2_samples + x3_samples
############################################################################

    # filtrn amostr com soma igul a uma constn (por exmplo, s = 10)
    filtered_indices = np.where(s_samples == S_value)[0]
###################################################################################

    # contgm de evnts (x1, x2, x3) para esss amostr
    filtered_x1 = x1_samples[filtered_indices]
    filtered_x2 = x2_samples[filtered_indices]
    filtered_x3 = x3_samples[filtered_indices]
#####################################################################################

    # contnd as ocorrn (x1, x2, x3)
    event_counts = Counter(zip(filtered_x1, filtered_x2, filtered_x3))
##############################################################

    # calcln a distrb multnm terca para o mesmo valr de s
    total_events = len(filtered_indices)  # numro de evnts com s = 10
    prob_1 = lambda_1 / (lambda_1 + lambda_2 + lambda_3)
    prob_2 = lambda_2 / (lambda_1 + lambda_2 + lambda_3)
    prob_3 = lambda_3 / (lambda_1 + lambda_2 + lambda_3)
###########################################################################

    # calcln a probbl terca usndo a formla multnm
    theoretical_probs = {}
    for (k1, k2, k3), count in event_counts.items():
        theoretical_prob = multinomial.pmf([k1, k2, k3], S_value, [prob_1, prob_2, prob_3])
        theoretical_probs[(k1, k2, k3)] = theoretical_prob
#######################################################################################

    # comprc entre as probbl simlds e teorcs
    simulated_probs = {k: v / total_events for k, v in event_counts.items()}
##########################################################################################

    # calcln o mse
    mse = np.mean([(simulated_probs.get(k, 0) - theoretical_probs.get(k, 0))**2 for k in set(simulated_probs) | set(theoretical_probs)])
    mse_list.append(mse)
######################################################################

# plotnd o mse em escla logrtm
plt.plot(num_simulations_list, mse_list, marker='o')
plt.xscale('log')
plt.xlabel('Número de Simulações (log escala)')
plt.ylabel('MSE')
plt.title('MSE vs. Número de Simulações')
plt.show()

n_intervals = 100
zones = 4
categories = 2
true_lambda = np.array([[8.0, .2, 1.0, 4.5], [0.5, .45, 1.5, .103]])

zone_sizes = np.array([1.0, 2.0, 1.5, 0.5])
normalized_zone_sizes = zone_sizes / np.sum(zone_sizes)

true_p = np.array([
    normalized_zone_sizes * np.random.uniform(0.5, 1.5),
    normalized_zone_sizes * np.random.uniform(0.5, 1.5)
])

epsilon = 1e-6

n_observations = 5
M_c_i_t_n_1 = np.zeros((categories, zones, n_intervals, n_observations), dtype=int)
M_c_t_n_0 = np.zeros((categories, zones, n_intervals, n_observations), dtype=int)

for c in range(categories):
    for t in range(n_intervals):
        for n in range(n_observations):
            S_c_t = np.sum(true_lambda[c, :])
            Y_c_i_t_n = np.array([np.random.poisson((1 - true_p[c, i]) * true_lambda[c, i]) for i in range(zones)])
            M_c_i_t_n_1[c, :, t, n] = Y_c_i_t_n
            for i in range(zones):
                Z_c_t_n = np.random.poisson(true_p[c, i] * true_lambda[c, i])
                M_c_t_n_0[c, i, t, n] = Z_c_t_n

M_c_t_0 = np.sum(M_c_t_n_0, axis=3)
M_c_i_t_1 = np.sum(M_c_i_t_n_1, axis=3)
M_0 = np.sum(M_c_t_0, axis=2)
M_1 = np.sum(M_c_i_t_1, axis=2)

def log_likelihood(estimated_lambda, estimated_p):
    log_like = 0
    for c in range(categories):
        for t in range(n_intervals):
            for i in range(zones):
                lambda_term = max((1 - estimated_p[c, i]) * estimated_lambda[c, i, t], epsilon)
                p_term = max(estimated_p[c, i] * estimated_lambda[c, i, t], epsilon)
                log_like += M_c_i_t_1[c, i, t] * np.log(lambda_term)
                log_like += M_c_t_0[c, i, t] * np.log(p_term)
    return log_like

def em_algorithm_s(max_iterations=100, tolerance=1e-6, learning_rate=0.1):
    log_likelihoods = []
    trajectory_lambda = []
    trajectory_p = []
    estimated_lambda = np.random.rand(categories, zones, n_intervals)
    estimated_p = np.random.uniform(low=0.2, high=0.8, size=(categories, zones))

    for iteration in range(max_iterations):
        expected_Z_c_t = np.zeros((categories, zones, n_intervals))
        for c in range(categories):
            for t in range(n_intervals):
                for i in range(zones):
                    lambda_p_term = max(estimated_p[c, i] * estimated_lambda[c, i, t], epsilon)
                    expected_Z_c_t[c, :, t] = M_c_t_0[c, :, t] / lambda_p_term

        for c in range(categories):
            for i in range(zones):
                for t in range(n_intervals):
                    new_lambda = np.sum(M_c_i_t_1[c, i, t]) / (n_observations * (1 - estimated_p[c, i]))
                    estimated_lambda[c, i, t] = (1 - learning_rate) * estimated_lambda[c, i, t] + learning_rate * new_lambda

        for c in range(categories):
            for i in range(zones):
                numerator = np.sum(M_c_t_0[c, :, :])
                denominator = numerator + np.sum(M_c_i_t_1[c, :, :])
                new_p = numerator / max(denominator, epsilon)
                estimated_p[c, i] =(1 - learning_rate) * estimated_p[c, i] + learning_rate * new_p

        current_log_likelihood = log_likelihood(estimated_lambda, estimated_p)
        log_likelihoods.append(current_log_likelihood)

        trajectory_lambda.append(np.mean(estimated_lambda))
        trajectory_p.append(np.mean(estimated_p))

        print(f"Iteração {iteration + 1}: Log-verossimilhança = {current_log_likelihood:.6f}")
        print(f"estimated_lambda (média): {np.mean(estimated_lambda)}")
        print(f"estimated_p (média): {np.mean(estimated_p)}")

        if iteration > 0 and abs(current_log_likelihood - previous_log_likelihood) < tolerance:
            print(f"Convergência atingida após {iteration + 1} iterações.")
            break

        previous_log_likelihood = current_log_likelihood

    return estimated_lambda, estimated_p, log_likelihoods,  trajectory_lambda, trajectory_p

estimated_lambda_s, estimated_p_s, log_likelihoods_s, trajectory_lambda, trajectory_p = em_algorithm_s()

print("Valores reais de p para cada categoria:")
print(true_p)

print("\nValores estimados de p para cada categoria:")
print(estimated_p_s)

print("\nMédia dos valores reais de lambda")
print(true_lambda)

mean_lambda = np.mean(estimated_lambda_s, axis=2)
print("\nMédia dos valores estimados de lambda ")
print(mean_lambda)

#@title experimentando com p_true conhecido:
import numpy as np

n_intervals = 100
zones = 4
categories = 2
true_lambda = np.array([[8.0, .2, 1.0, 4.5], [0.5, .45, 1.5, .103]])

zone_sizes = np.array([1.0, 2.0, 1.5, 0.5])
normalized_zone_sizes = zone_sizes / np.sum(zone_sizes)

true_p = np.array([
    normalized_zone_sizes * np.random.uniform(0.5, 1.5),
    normalized_zone_sizes * np.random.uniform(0.5, 1.5)
])

epsilon = 1e-6

n_observations = 5
M_c_i_t_n_1 = np.zeros((categories, zones, n_intervals, n_observations), dtype=int)
M_c_t_n_0 = np.zeros((categories, zones, n_intervals, n_observations), dtype=int)

for c in range(categories):
    for t in range(n_intervals):
        for n in range(n_observations):
            S_c_t = np.sum(true_lambda[c, :])
            Y_c_i_t_n = np.array([np.random.poisson((1 - true_p[c, i]) * true_lambda[c, i]) for i in range(zones)])
            M_c_i_t_n_1[c, :, t, n] = Y_c_i_t_n
            for i in range(zones):
                Z_c_t_n = np.random.poisson(true_p[c, i] * true_lambda[c, i])
                M_c_t_n_0[c, i, t, n] = Z_c_t_n

M_c_t_0 = np.sum(M_c_t_n_0, axis=3)
M_c_i_t_1 = np.sum(M_c_i_t_n_1, axis=3)
M_0 = np.sum(M_c_t_0, axis=2)
M_1 = np.sum(M_c_i_t_1, axis=2)

def log_likelihood(estimated_lambda, estimated_p):
    log_like = 0
    for c in range(categories):
        for t in range(n_intervals):
            for i in range(zones):
                lambda_term = max((1 - estimated_p[c, i]) * estimated_lambda[c, i, t], epsilon)
                p_term = max(estimated_p[c, i] * estimated_lambda[c, i, t], epsilon)
                log_like += M_c_i_t_1[c, i, t] * np.log(lambda_term)
                log_like += M_c_t_0[c, i, t] * np.log(p_term)
    return log_like

def em_algorithm_s(max_iterations=100, tolerance=1e-6, learning_rate=0.1):
    log_likelihoods = []
    trajectory_lambda = []
    trajectory_p = []
    estimated_lambda = np.random.rand(categories, zones, n_intervals)
    estimated_p = np.random.uniform(low=0.2, high=0.8, size=(categories, zones))

    for iteration in range(max_iterations):
        for c in range(categories):
            for i in range(zones):
                for t in range(n_intervals):
                    new_lambda = np.sum(M_c_i_t_1[c, i, t]) / (n_observations * (1 - true_p[c, i]))
                    estimated_lambda[c, i, t] = (1 - learning_rate) * estimated_lambda[c, i, t] + learning_rate * new_lambda

        for c in range(categories):
            for i in range(zones):
                numerator = np.sum(M_c_t_0[c, :, :])
                denominator = numerator + np.sum(M_c_i_t_1[c, :, :])
                new_p = numerator / max(denominator, epsilon)
                estimated_p[c, i] =true_p[c,i]#(1 - learning_rate) * estimated_p[c, i] + learning_rate * new_p

        current_log_likelihood = log_likelihood(estimated_lambda, true_p)
        log_likelihoods.append(current_log_likelihood)

        print(f"Iteração {iteration + 1}: Log-verossimilhança = {current_log_likelihood:.6f}")
        print(f"estimated_lambda (média): {np.mean(estimated_lambda)}")
        print(f"estimated_p (média): {np.mean(estimated_p)}")

        if iteration > 0 and abs(current_log_likelihood - previous_log_likelihood) < tolerance:
            print(f"Convergência atingida após {iteration + 1} iterações.")
            break

        previous_log_likelihood = current_log_likelihood

    return estimated_lambda, estimated_p, log_likelihoods,  trajectory_lambda, trajectory_p

estimated_lambda_s, estimated_p_s, log_likelihoods_s, trajectory_lambda, trajectory_p = em_algorithm_s()

print("Valores reais de p para cada categoria:")
print(true_p)

print("\nValores estimados de p para cada categoria:")
print(estimated_p_s)

print("\nMédia dos valores reais de lambda")
print(true_lambda)

mean_lambda = np.mean(estimated_lambda_s, axis=2)
print("\nMédia dos valores estimados de lambda ")
print(mean_lambda)

# prompt: give me the estimated_lambda where the likelihood is maxim

max_likelihood_index = log_likelihoods_s.index(max(log_likelihoods_s))
estimated_lambda_max_likelihood = estimated_lambda_s[max_likelihood_index]
print(estimated_lambda_max_likelihood)

# prompt: plot estimated_lambda[0,0] mean axis 2 evolution as likelihood increases

plt.figure(figsize=(10, 6))
plt.plot(np.mean(estimated_lambda_s[0, 0, :], axis=2), log_likelihoods_s)
plt.xlabel("estimated_lambda[0,0] mean (axis 2)")
plt.ylabel("Log-Verossimilhança")
plt.title("Evolução de estimated_lambda[0,0] com a Log-Verossimilhança")
plt.grid(True)
plt.show()

import numpy as np
import pandas as pd
##################################################################################

# parmtr
n_intervals = 100
zones = 4
categories = 2
n_observations = 5
epsilon = 1e-6
######################################################################################

# valrs reas de lambda e p
true_lambda = np.array([[8.0, .2, 1.0, 4.5], [0.5, .45, 1.5, .103]])
zone_sizes = np.array([1.0, 2.0, 1.5, 0.5])
normalized_zone_sizes = zone_sizes / np.sum(zone_sizes)
true_p = np.array([
    normalized_zone_sizes * np.random.uniform(0.5, 1.5),
    normalized_zone_sizes * np.random.uniform(0.5, 1.5)
])
#######################################################################################

# iniclz lists para armznr os dads
data = []
################################################################################

# gerndo os dads simlds
for c in range(categories):
    for t in range(n_intervals):
        for n in range(n_observations):
            S_c_t = np.sum(true_lambda[c, :])
            Y_c_i_t_n = [np.random.poisson((1 - true_p[c, i]) * true_lambda[c, i]) for i in range(zones)]
            Z_c_t_n = [np.random.poisson(true_p[c, i] * true_lambda[c, i]) for i in range(zones)]
#########################################################################

            # armznn cada obsrvc como um dicnr
            for i in range(zones):
                data.append({
                    'category': c,
                    'zone': i,
                    'interval': t,
                    'observation': n,
                    'Y_c_i_t_n': Y_c_i_t_n[i],
                    'Z_c_t_n': Z_c_t_n[i]
                })
##########################################################################

# convrt a lista de dicnrs em um datfrm do pands
df = pd.DataFrame(data)
#######################################################################

# crndo datfrm agrgds para substt as soms fets antrrm
M_c_t_0 = df.groupby(['category', 'zone', 'interval'])['Z_c_t_n'].sum().unstack(fill_value=0)
M_c_i_t_1 = df.groupby(['category', 'zone', 'interval'])['Y_c_i_t_n'].sum().unstack(fill_value=0)
M_0 = M_c_t_0.sum(level='category')
M_1 = M_c_i_t_1.sum(level='category')
####################################################################

# funco de verssm logrtm adaptd para usar o datfrm
def log_likelihood(estimated_lambda, estimated_p):
    log_like = 0
    for c in range(categories):
        for t in range(n_intervals):
            for i in range(zones):
                lambda_term = max((1 - estimated_p[c, i]) * estimated_lambda[c, i, t], epsilon)
                p_term = max(estimated_p[c, i] * estimated_lambda[c, i, t], epsilon)
                log_like += M_c_i_t_1.loc[(c, i), t] * np.log(lambda_term)
                log_like += M_c_t_0.loc[(c, i), t] * np.log(p_term)
    return log_like
################################################################

# funco do algrtm em adaptd
def em_algorithm_s(max_iterations=100, tolerance=1e-6, learning_rate=0.1):
    log_likelihoods = []
    trajectory_lambda = []
    trajectory_p = []
    estimated_lambda = np.random.rand(categories, zones, n_intervals)
    estimated_p = np.random.uniform(low=0.2, high=0.8, size=(categories, zones))
#######################################################################

    for iteration in range(max_iterations):
        expected_Z_c_t = np.zeros((categories, zones, n_intervals))
        for c in range(categories):
            for t in range(n_intervals):
                for i in range(zones):
                    lambda_p_term = max(estimated_p[c, i] * estimated_lambda[c, i, t], epsilon)
                    expected_Z_c_t[c, :, t] = M_c_t_0.loc[(c, i), t] / lambda_p_term
######################################################################################

        for c in range(categories):
            for i in range(zones):
                for t in range(n_intervals):
                    new_lambda = np.sum(M_c_i_t_1.loc[(c, i), t]) / (n_observations * (1 - estimated_p[c, i]))
                    estimated_lambda[c, i, t] = (1 - learning_rate) * estimated_lambda[c, i, t] + learning_rate * new_lambda
##########################################################################################

        for c in range(categories):
            for i in range(zones):
                numerator = M_c_t_0.loc[(c, i), :].sum()
                denominator = numerator + M_c_i_t_1.loc[(c, i), :].sum()
                new_p = numerator / max(denominator, epsilon)
                estimated_p[c, i] =(1 - learning_rate) * estimated_p[c, i] + learning_rate * new_p
############################################################################

        current_log_likelihood = log_likelihood(estimated_lambda, estimated_p)
        log_likelihoods.append(current_log_likelihood)
###############################################################################

        trajectory_lambda.append(np.mean(estimated_lambda))
        trajectory_p.append(np.mean(estimated_p))
################################################################################################

        print(f"Iteração {iteration + 1}: Log-verossimilhança = {current_log_likelihood:.6f}")
        print(f"estimated_lambda (média): {np.mean(estimated_lambda)}")
        print(f"estimated_p (média): {np.mean(estimated_p)}")
###########################################################################

        if iteration > 0 and abs(current_log_likelihood - previous_log_likelihood) < tolerance:
            print(f"Convergência atingida após {iteration + 1} iterações.")
            break
###############################################################################################

        previous_log_likelihood = current_log_likelihood
####################################################################################

    return estimated_lambda, estimated_p, log_likelihoods, trajectory_lambda, trajectory_p
################################################################

# rodndo o algrtm em
estimated_lambda_s, estimated_p_s, log_likelihoods_s, trajectory_lambda, trajectory_p = em_algorithm_s()
#############################################################################

print("Valores reais de p para cada categoria:")
print(true_p)
##############################################################

print("\nValores estimados de p para cada categoria:")
print(estimated_p_s)
########################################################################

print("\nMédia dos valores reais de lambda")
print(true_lambda)
#######################################################################

mean_lambda = np.mean(estimated_lambda_s, axis=2)
print("\nMédia dos valores estimados de lambda ")
print(mean_lambda)

#################################################################

n_intervals = 1000
zones = 4
categories = 2
true_lambda = np.array([[8.0, .2, 1.0, 4.5], [0.5, .45, 1.5, .103]])
###############################################################

zone_sizes = np.array([1.0, 2.0, 1.5, 0.5])
normalized_zone_sizes = zone_sizes / np.sum(zone_sizes)
###############################################################################

true_p = np.array([
    normalized_zone_sizes * np.random.uniform(0.5, 1.5),
    normalized_zone_sizes * np.random.uniform(0.5, 1.5)
])
##############################################################################

epsilon = 1e-6
######################################################################################

n_observations = 5
M_c_i_t_n_1 = np.zeros((categories, zones, n_intervals, n_observations), dtype=int)
M_c_t_n_0 = np.zeros((categories, zones, n_intervals, n_observations), dtype=int)
#######################################################################################

for c in range(categories):
    for t in range(n_intervals):
        for n in range(n_observations):
            S_c_t = np.sum(true_lambda[c, :])
            Y_c_i_t_n = np.array([np.random.poisson((1 - true_p[c, i]) * true_lambda[c, i]) for i in range(zones)])
            M_c_i_t_n_1[c, :, t, n] = Y_c_i_t_n
            for i in range(zones):
                Z_c_t_n = np.random.poisson(true_p[c, i] * true_lambda[c, i])
                M_c_t_n_0[c, i, t, n] = Z_c_t_n
########################################################################

M_c_t_0 = np.sum(M_c_t_n_0, axis=3)
M_c_i_t_1 = np.sum(M_c_i_t_n_1, axis=3)
M_0 = np.sum(M_c_t_0, axis=2)
M_1 = np.sum(M_c_i_t_1, axis=2)
###############################################################

def log_likelihood(estimated_lambda, estimated_p):
    log_like = 0
    for c in range(categories):
        for t in range(n_intervals):
            for i in range(zones):
                lambda_term = max((1 - estimated_p[c, i]) * estimated_lambda[c, i, t], epsilon)
                p_term = max(estimated_p[c, i] * estimated_lambda[c, i, t], epsilon)
                log_like += M_c_i_t_1[c, i, t] * np.log(lambda_term)
                log_like += M_c_t_0[c, i, t] * np.log(p_term)
    return log_like
############################################################

def em_algorithm_s(max_iterations=100, tolerance=1e-6, learning_rate=0.1):
    log_likelihoods = []
    trajectory_lambda = []
    trajectory_p = []
    estimated_lambda = np.random.rand(categories, zones, n_intervals)
    estimated_p = np.random.uniform(low=0.2, high=0.8, size=(categories, zones))
###########################################################################

    for iteration in range(max_iterations):
        expected_Z_c_t = np.zeros((categories, zones, n_intervals))
###############################################################

        # etpa e: calclo da expctt para as varvs oclts
        for c in range(categories):
            for t in range(n_intervals):
                for i in range(zones):
                    lambda_p_term = max(estimated_p[c, i] * estimated_lambda[c, i, t], epsilon)
                    expected_Z_c_t[c, i, t] = M_c_t_0[c, i, t] / lambda_p_term
##################################################################################

        # etpa m: maxmzc da funco de verssm
        for c in range(categories):
            for i in range(zones):
                for t in range(n_intervals):
                    new_lambda = np.sum(M_c_i_t_1[c, i, t]) / (n_observations * (1 - estimated_p[c, i]))
                    estimated_lambda[c, i, t] = (1 - learning_rate) * estimated_lambda[c, i, t] + learning_rate * new_lambda
#######################################################################################

        for c in range(categories):
            for t in range(n_intervals):
                # atulzc de p para cada zona seprdm
                numerator = np.sum(M_c_t_0[c, :, t])  # apns para a zona i
                denominator = numerator + np.sum(M_c_i_t_1[c, :, t])  # apns para a zona i
                new_p = numerator / max(denominator, epsilon)
                estimated_p[c, i] = (1 - learning_rate) * estimated_p[c, i] + learning_rate * new_p
############################################################################################

        # calclo da log-verssm
        current_log_likelihood = log_likelihood(estimated_lambda, estimated_p)
        log_likelihoods.append(current_log_likelihood)
######################################################################

        trajectory_lambda.append(np.mean(estimated_lambda))
        trajectory_p.append(np.mean(estimated_p))
#####################################################################

        print(f"Iteração {iteration + 1}: Log-verossimilhança = {current_log_likelihood:.6f}")
        print(f"estimated_lambda (média): {np.mean(estimated_lambda)}")
        print(f"estimated_p (média): {np.mean(estimated_p)}")
############################################################################################

        if iteration > 0 and abs(current_log_likelihood - previous_log_likelihood) < tolerance:
            print(f"Convergência atingida após {iteration + 1} iterações.")
            break
############################################################################################

        previous_log_likelihood = current_log_likelihood
##############################################################

    return estimated_lambda, estimated_p, log_likelihoods, trajectory_lambda, trajectory_p
################################################################################

####################################################################

estimated_lambda_s, estimated_p_s, log_likelihoods_s, trajectory_lambda, trajectory_p = em_algorithm_s()
###############################################################

print("Valores reais de p para cada categoria:")
print(true_p)
################################################################################################

print("\nValores estimados de p para cada categoria:")
print(estimated_p_s)
#########################################################################

print("\nMédia dos valores reais de lambda")
print(true_lambda)
#############################################################################

mean_lambda = np.mean(estimated_lambda_s, axis=2)
print("\nMédia dos valores estimados de lambda ")
print(mean_lambda)
################################################################

###########################################################################################

numerator = np.sum(M_c_t_0[c, :, :])
denominator = numerator + np.sum(M_c_i_t_1[c, :, :])
numerator / denominator